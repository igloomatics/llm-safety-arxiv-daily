base_url: "https://arxiv.paperswithcode.com/api/v0/papers/"
user_name: "Xuchen-Li"
repo_name: "llm-safety-arxiv-daily"
show_authors: True
show_links: True
max_results: 10

publish_readme: True
publish_gitpage: True

json_readme_path: './docs/llm-arxiv-daily.json'
json_gitpage_path: './docs/llm-arxiv-daily-web.json'

md_readme_path: 'README.md'
md_gitpage_path: './docs/index.md'

# keywords to search
keywords:
    "LLM Safety":
        filters: ["LLM Jailbreak", "Prompt Injection", "Adversarial Attack LLM", "LLM Backdoor", "Red Teaming LLM"]
    "LLM Alignment":
        filters: ["LLM Alignment", "RLHF Safety", "LLM Machine Unlearning", "LLM Safety Guardrails"]
    "LLM Hallucination":
        filters: ["LLM Hallucination", "LLM Factuality", "LLM Sycophancy"]
    "LLM Privacy":
        filters: ["LLM Privacy", "LLM Toxicity", "LLM Bias"]